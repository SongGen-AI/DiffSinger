{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea881a6-55a9-4f17-8a9c-a73aae4c94b5",
   "metadata": {
    "id": "eexZl_OCDmQ3"
   },
   "source": [
    "# [Train_Acoustic_1] **Preprocess data for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7bb33c-15e6-4d33-909a-91c115628369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data\n",
    "%cd /workspace/content\n",
    "# This cell will create a folder name [raw_data] in the root folder and extract your data into it\n",
    "\n",
    "data_type = \"lab + wav (NNSVS format)\" # @param [\"lab + wav (NNSVS format)\", \"csv + wav (DiffSinger format)\", \"ds + wav (DiffSinger format)\"]\n",
    "\n",
    "# The path to your data zip file\n",
    "data_zip_path = \"/workspace/diffsinger_dataset.zip\" #@param {type:\"string\"}\n",
    "\n",
    "# nnsvs-db-converter settings (lab + wav ONLY)\n",
    "# These values can exceed the amount that's in your data to maximize the segment length or to keep the data as is_\n",
    "# This option is necessary for variance's pitch training\n",
    "# Turn it on as you want to train Variance first. Turn it off later for Acoustics.\n",
    "\n",
    "estimate_midi = False #@param {type:\"boolean\"}\n",
    "\n",
    "# Determine how long it will segment your data to based on silence phoneme placement (seconds)\n",
    "segment_length = 15 #@param {type:\"slider\", min:5, max:35, step:1}\n",
    "\n",
    "#Determine how many silence phoneme is allowed in the middle of each segment\n",
    "max_silence_phoneme_amount = 1 #@param {type:\"slider\", min:0, max:50, step:1}\n",
    "\n",
    "# leaving -S at 60 so max silence can be 60 seconds that exceeds the segment legnth cap idk why///\n",
    "# making the segment length cap at 35 secs because any longer than that would make training goes really slow\n",
    "\n",
    "# my ass dont remember why i made two... i think one is unnecessary extra but mehhh\n",
    "all_shits = \"/workspace/content/raw_data\"\n",
    "all_shits_not_wav_n_lab = \"/workspace/content/raw_data/diffsinger_db\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "from pydub import AudioSegment\n",
    "\n",
    "if os.path.exists(\"/workspace/content/raw_data\"):\n",
    "    shutil.rmtree(\"/workspace/content/raw_data\")\n",
    "\n",
    "if not os.path.exists(all_shits_not_wav_n_lab):\n",
    "  os.makedirs(all_shits_not_wav_n_lab)\n",
    "\n",
    "# using 'if not' bc i edited the wrong section which im also too lazy to fix it <3\n",
    "if not data_type == \"lab + wav (NNSVS format)\":\n",
    "    #changed to 7zip to support more compression types\n",
    "    !7z x \"$data_zip_path\" -o{all_shits_not_wav_n_lab}\n",
    "    for root, dirs, files in os.walk(all_shits):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".lab\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, \"r\") as file:\n",
    "                    file_data = file.read()\n",
    "                file_data = file_data.replace(\"SP\", \"pau\")\n",
    "                file_data = file_data.replace(\"br\", \"AP\")\n",
    "                with open(file_path, \"w\") as file:\n",
    "                    file.write(file_data)\n",
    "\n",
    "else:\n",
    "    !7z x \"$data_zip_path\" -o{all_shits_not_wav_n_lab}\n",
    "\n",
    "\n",
    "# for funny auto dict generator lmao\n",
    "out = \"/workspace/content/DiffSinger/dictionaries/custom_dict.txt\"\n",
    "\n",
    "phonemes = set()\n",
    "\n",
    "def is_excluded(phoneme):\n",
    "    return phoneme in [\"pau\", \"AP\", \"SP\", \"sil\"]\n",
    "\n",
    "if data_type == \"lab + wav (NNSVS format)\":\n",
    "    phoneme_folder_path = all_shits\n",
    "    for root, dirs, files in os.walk(phoneme_folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".lab\"):\n",
    "                fpath = os.path.join(root, file)\n",
    "                with open(fpath, \"r\") as lab_file:\n",
    "                    for line in lab_file:\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            phoneme = line.split()[2]\n",
    "                            if not is_excluded(phoneme):\n",
    "                                phonemes.add(phoneme)\n",
    "elif data_type == \"csv + wav (DiffSinger format)\":\n",
    "    phoneme_folder_path = all_shits_not_wav_n_lab\n",
    "    for root, dirs, files in os.walk(phoneme_folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                fpath = os.path.join(root, file)\n",
    "                with open(fpath, \"r\", newline=\"\") as csv_file:\n",
    "                    csv_reader = csv.DictReader(csv_file)\n",
    "                    for row in csv_reader:\n",
    "                        if \"ph_seq\" in row:\n",
    "                            ph_seq = row[\"ph_seq\"].strip()\n",
    "                            for phoneme in ph_seq.split():\n",
    "                                if not is_excluded(phoneme):\n",
    "                                    phonemes.add(phoneme)\n",
    "\n",
    "else:\n",
    "    phoneme_folder_path = all_shits_not_wav_n_lab\n",
    "    for root, dirs, files in os.walk(phoneme_folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".ds\"):\n",
    "                fpath = os.path.join(root, file)\n",
    "                with open(fpath, \"r\") as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    for entry in data:\n",
    "                        if \"ph_seq\" in entry:\n",
    "                            ph_seq = entry[\"ph_seq\"].strip()\n",
    "                            phoneme_list = ph_seq.split()\n",
    "                            for phoneme in phoneme_list:\n",
    "                                if not is_excluded(phoneme):\n",
    "                                    phonemes.add(phoneme)\n",
    "\n",
    "with open(out, \"w\") as f:\n",
    "    for phoneme in sorted(phonemes):\n",
    "        f.write(phoneme + \"\t\" + phoneme + \"\\n\")\n",
    "\n",
    "# for vowels and consonants.txt.... well adding luquid type for uta's script\n",
    "dict_path = out\n",
    "vowel_types = {\"a\", \"i\", \"u\", \"e\", \"o\", \"N\", \"M\", \"NG\"}\n",
    "liquid_types = {\"y\", \"w\", \"l\", \"r\"} # r for english labels, it should be fine with jp too\n",
    "vowel_data = []\n",
    "consonant_data = []\n",
    "liquid_data = []\n",
    "\n",
    "with open(dict_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        phoneme, _ = line.strip().split(\"\\t\")\n",
    "        if phoneme[0] in vowel_types:\n",
    "            vowel_data.append(phoneme)\n",
    "        elif phoneme[0] in liquid_types:\n",
    "            liquid_data.append(phoneme)\n",
    "        else:\n",
    "            consonant_data.append(phoneme)\n",
    "\n",
    "vowel_data.sort()\n",
    "liquid_data.sort()\n",
    "consonant_data.sort()\n",
    "directory = os.path.dirname(dict_path)\n",
    "\n",
    "# make txt for language json file\n",
    "vowel_txt_path = os.path.join(directory, \"vowels.txt\")\n",
    "with open(vowel_txt_path, \"w\") as f:\n",
    "    f.write(\" \".join(vowel_data))\n",
    "liquid_txt_path = os.path.join(directory, \"liquids.txt\")\n",
    "with open(liquid_txt_path, \"w\") as f:\n",
    "    f.write(\" \".join(liquid_data))\n",
    "consonant_txt_path = os.path.join(directory, \"consonants.txt\")\n",
    "with open(consonant_txt_path, \"w\") as f:\n",
    "    f.write(\" \".join(consonant_data))\n",
    "\n",
    "\n",
    "# here's a funny json append\n",
    "with open(vowel_txt_path, \"r\") as f:\n",
    "    vowel_data = f.read().split()\n",
    "with open(liquid_txt_path, \"r\") as f:\n",
    "    liquid_data = f.read().split()\n",
    "with open(consonant_txt_path, \"r\") as f:\n",
    "    consonant_data = f.read().split()\n",
    "phones4json = {\"vowels\": vowel_data, \"liquids\": liquid_data}\n",
    "with open(\"/workspace/content/nnsvs-db-converter/lang.sample.json\", \"w\") as rawr:\n",
    "    json.dump(phones4json, rawr, indent=4)\n",
    "\n",
    "\n",
    "if data_type == \"lab + wav (NNSVS format)\":\n",
    "    db_converter_script = \"/workspace/content/nnsvs-db-converter/db_converter.py\"\n",
    "    for raw_folder_name in os.listdir(all_shits_not_wav_n_lab):\n",
    "        raw_folder_path = os.path.join(all_shits_not_wav_n_lab, raw_folder_name)\n",
    "        if os.path.isdir(raw_folder_path):\n",
    "            if estimate_midi:\n",
    "                !python {db_converter_script} -s {max_silence_phoneme_amount} -S 60 -l {segment_length} -m -c -L \"/workspace/content/nnsvs-db-converter/lang.sample.json\" --folder {raw_folder_path}\n",
    "            else:\n",
    "                #!python {db_converter_script} -s 2 --folder {raw_folder_path} 2> /dev/null\n",
    "                !python {db_converter_script} -s {max_silence_phoneme_amount} -S 60 -l {segment_length} -L \"/workspace/content/nnsvs-db-converter/lang.sample.json\" --folder {raw_folder_path}\n",
    "\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "if data_type == \"lab + wav (NNSVS format)\":\n",
    "    for raw_folder_name in os.listdir(all_shits_not_wav_n_lab):\n",
    "        raw_folder_path = os.path.join(all_shits_not_wav_n_lab, raw_folder_name)\n",
    "        !rm -rf {raw_folder_path}/*.wav {raw_folder_path}/*.lab\n",
    "        !mv {raw_folder_path}/diffsinger_db/* {raw_folder_path} 2> /dev/null\n",
    "        !rm -rf {raw_folder_path}/diffsinger_db\n",
    "        #!cp {raw_folder_path}/wavs/*.wav {raw_folder_path}\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# make it replace the first SP to AP cus it seems like people always forgot about it\n",
    "for root, _, files in os.walk(all_shits_not_wav_n_lab):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, \"r\", newline=\"\") as input_file:\n",
    "                csv_reader = csv.reader(input_file)\n",
    "                data = [row for row in csv_reader]\n",
    "                header = data[0]\n",
    "                if \"ph_seq\" in header:\n",
    "                    ph_seq_index = header.index(\"ph_seq\")\n",
    "                    if len(data) > 1 and len(data[1]) > ph_seq_index:\n",
    "                        data[1][ph_seq_index] = data[1][ph_seq_index].replace(\"SP\", \"AP\", 1)\n",
    "            with open(file_path, \"w\", newline=\"\") as output_file:\n",
    "                csv_writer = csv.writer(output_file)\n",
    "                csv_writer.writerows(data)\n",
    "\n",
    "print(\"extraction complete!\")\n",
    "print(\"|\")\n",
    "print(\"|\")\n",
    "print(\"|\")\n",
    "print(\"I'm also nice enough to convert your data and also write your dict.txt lmao. You are welcome :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd919ac-87aa-46fa-bbe0-264afaccad8c",
   "metadata": {
    "id": "eexZl_OCDmQ3"
   },
   "source": [
    "# [Train_Acoustic_2] **Edit Config for training Acoustic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac9cbde-339c-451d-b491-30b72e9742cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config updated! see below for config's information\n",
      "|\n",
      "|\n",
      "|\n",
      "+++---ACOUSTIC SINGLE-SPEAKER TRAINING---+++\n",
      "|\n",
      "|\n",
      "|\n",
      "+++---user's settings---+++\n",
      "\n",
      "\n",
      "speaker name: ['data']\n",
      "\n",
      "\n",
      "data augmentation: True\n",
      "\n",
      "\n",
      "pitch extractor: rmvpe\n",
      "\n",
      "\n",
      "binary data save directory: /workspace/content/DiffSinger/MyModel_Acoustic/binary\n",
      "\n",
      "\n",
      "your model will be saved to: /workspace/content/DiffSinger/MyModel_Acoustic\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "+++---other auto-defined settings---+++\n",
      "\n",
      "\n",
      "test files (auto selected): ['tiger_023_seg000', 'tiger_019_seg002', 'tiger_027_seg011']\n",
      "\n",
      "\n",
      "dictionary (auto generated): custom_dict.txt\n",
      "\n",
      "\n",
      "max_batch_size: 12\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "if you don't like or disagree with any of these options,\n",
      "you can go and edit the config at [/workspace/content/DiffSinger/configs/acoustic.yaml]\n"
     ]
    }
   ],
   "source": [
    "#@title #Edit Config for Acoustic\n",
    "#@markdown ___\n",
    "\n",
    "import re\n",
    "import os\n",
    "import yaml\n",
    "import random #for the random test files lmaoz\n",
    "\n",
    "%cd /workspace/content\n",
    "clear_output()\n",
    "#@markdown <font size=\"-1.5\"> The training type you want to do\n",
    "config_type = \"acoustic\" # @param [\"acoustic\", \"variance\"]\n",
    "config_cap = config_type.upper()\n",
    "\n",
    "spk_name = [folder_name for folder_name in os.listdir(all_shits_not_wav_n_lab) if os.path.isdir(os.path.join(all_shits_not_wav_n_lab, folder_name))]\n",
    "# i used spk_name for something else cus i forgor now imma just copy and paste it\n",
    "spk_names = [folder_name for folder_name in os.listdir(all_shits_not_wav_n_lab) if os.path.isdir(os.path.join(all_shits_not_wav_n_lab, folder_name))]\n",
    "num_spk = len(spk_name)\n",
    "raw_dir = []\n",
    "for folder_name in spk_name:\n",
    "    folder_path = os.path.join(all_shits_not_wav_n_lab, folder_name)\n",
    "    raw_dir.append(folder_path)\n",
    "if num_spk == 1:\n",
    "    singer_type = \"SINGLE-SPEAKER\"\n",
    "    diff_loss_type = \"l2\"\n",
    "    f0_emb = \"continuous\"\n",
    "    use_spk_id = False\n",
    "    all_wav_files = []\n",
    "    for root, dirs, files in os.walk(\"/workspace/content/raw_data/diffsinger_db\"):\n",
    "        for file in files:\n",
    "            if file.endswith((\".wav\", \".ds\")):\n",
    "                full_path = os.path.join(root, file)\n",
    "                all_wav_files.append(full_path)\n",
    "    random.shuffle(all_wav_files)\n",
    "    random_ass_wavs = all_wav_files[:3]\n",
    "    random_ass_test_files = [os.path.splitext(os.path.basename(file))[0] for file in random_ass_wavs]\n",
    "\n",
    "else:\n",
    "    singer_type = \"MULTI-SPEAKER\"\n",
    "    diff_loss_type = \"l1\"\n",
    "    f0_emb = \"discrete\"\n",
    "    use_spk_id = True\n",
    "    folder_to_id = {folder_name: i for i, folder_name in enumerate(spk_name)}\n",
    "    random_ass_test_files = []\n",
    "    for folder_path in raw_dir:\n",
    "        if data_type == \"ds + wav (DiffSinger format)\":\n",
    "            audio_files = [f[:-4] for f in os.listdir(folder_path) if f.endswith(\".ds\")]\n",
    "        else:\n",
    "            audio_files = [f[:-4] for f in os.listdir(folder_path + \"/wavs\") if f.endswith(\".wav\")]\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        folder_id = folder_to_id.get(folder_name, -1)\n",
    "        prefixed_audio_files = [f\"{folder_id}:{audio_file}\" for audio_file in audio_files]\n",
    "        random_ass_test_files.extend(prefixed_audio_files[:3])\n",
    "spk_id = []\n",
    "for i, spk_name in enumerate(spk_name):\n",
    "    spk_id_format = f\"{i}:{spk_name}\"\n",
    "    spk_id.append(spk_id_format)\n",
    "\n",
    "#@markdown <font size=\"-1.5\"> Shallow Diffusion training for acoustic\n",
    "use_shallow_diffusion = \"true | gt_val\" # @param [\"false\", \"true | aux_val\", \"true | gt_val\"]\n",
    "if use_shallow_diffusion == \"false\":\n",
    "    shallow = False\n",
    "    gt_shallow = False\n",
    "elif use_shallow_diffusion == \"true | aux_val\":\n",
    "    shallow = True\n",
    "    gt_shallow = False\n",
    "else:\n",
    "    shallow = True\n",
    "    gt_shallow = True\n",
    "\n",
    "#@markdown <font size=\"-1.5\"> Half precision, or mixed precision can result in improved performance, achieving speedups on training (from [doc](https://lightning.ai/docs/pytorch/stable/common/trainer.html#precision))\n",
    "# the reason why i dont add 64 is because colab is already dreadfully slow at 32 so yes im leaving it out\n",
    "precision = \"16-mixed\" # @param [\"32-true\", \"bf16-mixed\", \"16-mixed\", \"bf16\", \"16\"]\n",
    "\n",
    "#@markdown <font size=\"-1.5\"> batch size setting, too low can cause bottleneck, too high can cause oom\n",
    "batch_size = 12 # @param {type:\"slider\", min:1, max:100, step:1}\n",
    "\n",
    "\n",
    "#@markdown <font size=\"-1.5\"> Your model save path\n",
    "save_dir = \"/workspace/content/DiffSinger/MyModel_Acoustic\" #@param {type:\"string\"}\n",
    "\n",
    "binary_save_dir = save_dir + \"/binary\"\n",
    "\n",
    "conf_dir = save_dir\n",
    "\n",
    "#@markdown ....................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "#@markdown <font size=\"-1.5\"> Option to use base model for finetuning\n",
    "\n",
    "enable_finetuning = False # @param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "#@markdown <font size=\"-1.5\"> Path to custom base model, leave blank to use [default](https://github.com/haru0l/diffsinger_models) models\n",
    "#wtf haru i just looked at your readme\"\"\"\"\"\n",
    "\n",
    "base_model_path = \"\" # @param {type:\"string\"}\n",
    "\n",
    "if enable_finetuning:\n",
    "    pretrain = True\n",
    "    if base_model_path:\n",
    "        pretrain_ckpt = base_model_path\n",
    "    else:\n",
    "        pretrain_ckpt = f\"/workspace/content/pretrain_models/{config_type}_pretrain.ckpt\"\n",
    "    finetune_strict_shapes = False\n",
    "    finetune_ckpt_path = pretrain_ckpt\n",
    "else:\n",
    "    pretrain = False\n",
    "    finetune_strict_shapes = True #default value\n",
    "    finetune_ckpt_path = None #default value\n",
    "\n",
    "#@markdown ....................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "#@markdown <font size=\"-1.5\"> Model embeds check; Tension, Energy, Breathiness, Voicing | for both acoustic and variance\n",
    "enable_embeds = True #@param {type: \"boolean\"}\n",
    "if enable_embeds:\n",
    "  tension_training = True\n",
    "  energy_training = True\n",
    "  breathiness_training = True\n",
    "  voicing_training = True\n",
    "else:\n",
    "  tension_training = False\n",
    "  energy_training = False\n",
    "  breathiness_training = False\n",
    "  voicing_training = False\n",
    "\n",
    "pitch_training = \"False\"\n",
    "if pitch_training == \"False\":\n",
    "    pitch_training = False\n",
    "    use_melody_encoder = False\n",
    "    use_glide_embed = False\n",
    "elif pitch_training == \"True | Standard\":\n",
    "    pitch_training = True\n",
    "    use_melody_encoder = False\n",
    "    use_glide_embed = False\n",
    "elif pitch_training == \"True | Standard + Glide\":\n",
    "    pitch_training = True\n",
    "    use_melody_encoder = False\n",
    "    use_glide_embed = True\n",
    "elif pitch_training == \"True | MelodyEncoder\":\n",
    "    pitch_training = True\n",
    "    use_melody_encoder = True\n",
    "    use_glide_embed = False\n",
    "else:\n",
    "    pitch_training = True\n",
    "    use_melody_encoder = True\n",
    "    use_glide_embed = True\n",
    "\n",
    "duration_training = True #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown ....................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "#@markdown <font size=\"-1.5\"> Pitch extractor algorithm\n",
    "\n",
    "f0_ext = \"rmvpe\" # @param [\"parselmouth\", \"rmvpe\", \"harvest\"]\n",
    "if f0_ext == \"rmvpe\":\n",
    "    pe_ckpt_pth = \"checkpoints/rmvpe/model.pt\"\n",
    "else:\n",
    "    pe_ckpt_pth = None\n",
    "\n",
    "#@markdown <font size=\"-1.5\"> Select this if you want to use data augmentation (default pitch shift and time stretch values)\n",
    "data_aug = True # Acoustic\n",
    "\n",
    "with open(\"/workspace/content/DiffSinger/configs/base.yaml\", \"r\") as config:\n",
    "    mother = yaml.safe_load(config)\n",
    "mother[\"pl_trainer_precision\"] = precision\n",
    "with open(\"/workspace/content/DiffSinger/configs/base.yaml\", \"w\") as config:\n",
    "    yaml.dump(mother, config)\n",
    "\n",
    "if  data_type == \"ds + wav (DiffSinger format)\":\n",
    "    prefer_ds = True\n",
    "else:\n",
    "    prefer_ds = False\n",
    "\n",
    "if config_type == \"acoustic\":\n",
    "    with open(\"/workspace/content/DiffSinger/configs/acoustic.yaml\", \"r\") as config:\n",
    "        bitch_ass_config = yaml.safe_load(config)\n",
    "    bitch_ass_config[\"speakers\"] = spk_names\n",
    "    bitch_ass_config[\"test_prefixes\"] = random_ass_test_files\n",
    "    bitch_ass_config[\"raw_data_dir\"] = raw_dir\n",
    "    bitch_ass_config[\"num_spk\"] = num_spk\n",
    "    bitch_ass_config[\"use_spk_id\"] = use_spk_id\n",
    "    #bitch_ass_config[\"spk_ids\"] = spk_id\n",
    "    bitch_ass_config[\"diff_loss_type\"] = diff_loss_type\n",
    "    bitch_ass_config[\"f0_embed_type\"] = f0_emb\n",
    "    bitch_ass_config[\"binary_data_dir\"] = binary_save_dir\n",
    "    bitch_ass_config[\"dictionary\"] = \"dictionaries/custom_dict.txt\"\n",
    "    bitch_ass_config[\"augmentation_args\"][\"random_pitch_shifting\"][\"enabled\"] = data_aug\n",
    "    bitch_ass_config[\"augmentation_args\"][\"random_time_stretching\"][\"enabled\"] = data_aug\n",
    "    bitch_ass_config[\"use_key_shift_embed\"] = data_aug\n",
    "    bitch_ass_config[\"use_speed_embed\"] = data_aug\n",
    "    bitch_ass_config[\"max_batch_size\"] = batch_size\n",
    "    bitch_ass_config[\"pe\"] = f0_ext\n",
    "    bitch_ass_config[\"use_energy_embed\"] = energy_training\n",
    "    bitch_ass_config[\"use_breathiness_embed\"] = breathiness_training\n",
    "    bitch_ass_config[\"use_tension_embed\"] = tension_training\n",
    "    bitch_ass_config[\"use_voicing_embed\"] = voicing_training\n",
    "\n",
    "    bitch_ass_config[\"pe_ckpt\"] = pe_ckpt_pth\n",
    "    #shallow diff stuff\n",
    "    bitch_ass_config[\"use_shallow_diffusion\"] = shallow\n",
    "    bitch_ass_config[\"shallow_diffusion_args\"][\"val_gt_start\"] = gt_shallow\n",
    "    #pretrain stuff\n",
    "    bitch_ass_config[\"finetune_enabled\"] = enable_finetuning\n",
    "    bitch_ass_config[\"finetune_ckpt_path\"] = finetune_ckpt_path\n",
    "    bitch_ass_config[\"finetune_strict_shapes\"] = finetune_strict_shapes\n",
    "\n",
    "    with open(\"/workspace/content/DiffSinger/configs/acoustic.yaml\", \"w\") as config:\n",
    "        yaml.dump(bitch_ass_config, config)\n",
    "else:\n",
    "    with open(\"/workspace/content/DiffSinger/configs/variance.yaml\", \"r\") as config:\n",
    "        bitch_ass_config = yaml.safe_load(config)\n",
    "    bitch_ass_config[\"speakers\"] = spk_names\n",
    "    bitch_ass_config[\"test_prefixes\"] = random_ass_test_files\n",
    "    bitch_ass_config[\"raw_data_dir\"] = raw_dir\n",
    "    bitch_ass_config[\"num_spk\"] = num_spk\n",
    "    bitch_ass_config[\"use_spk_id\"] = use_spk_id\n",
    "    bitch_ass_config[\"diff_loss_type\"] = diff_loss_type\n",
    "    bitch_ass_config[\"binary_data_dir\"] = binary_save_dir\n",
    "    bitch_ass_config[\"dictionary\"] = \"dictionaries/custom_dict.txt\"\n",
    "    bitch_ass_config[\"max_batch_size\"] = batch_size\n",
    "    bitch_ass_config[\"pe\"] = f0_ext # i think variance uses it for pitch ref as ground-truth for pitch training soooo\n",
    "    bitch_ass_config[\"pe_ckpt\"] = pe_ckpt_pth #same goes to this one\n",
    "\n",
    "    bitch_ass_config[\"predict_energy\"] = energy_training\n",
    "    bitch_ass_config[\"predict_breathiness\"] = breathiness_training\n",
    "    bitch_ass_config[\"predict_tension\"] = tension_training\n",
    "    bitch_ass_config[\"predict_pitch\"] = pitch_training\n",
    "    bitch_ass_config[\"predict_voicing\"] = voicing_training\n",
    "\n",
    "    bitch_ass_config[\"use_melody_encoder\"] = use_melody_encoder\n",
    "    bitch_ass_config[\"use_glide_embed\"] = use_glide_embed\n",
    "    bitch_ass_config[\"predict_dur\"] = duration_training\n",
    "    bitch_ass_config[\"binarization_args\"][\"prefer_ds\"] = prefer_ds\n",
    "    #pretrain stuff\n",
    "    bitch_ass_config[\"finetune_enabled\"] = enable_finetuning\n",
    "    bitch_ass_config[\"finetune_ckpt_path\"] = finetune_ckpt_path\n",
    "    bitch_ass_config[\"finetune_strict_shapes\"] = finetune_strict_shapes\n",
    "\n",
    "    with open(\"/workspace/content/DiffSinger/configs/variance.yaml\", \"w\") as config:\n",
    "        yaml.dump(bitch_ass_config, config)\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "with open(\"/workspace/content/DiffSinger/utils/hparams.py\", \"r\") as f:\n",
    "    hparams_py_read = f.read()\n",
    "hparams_py_read = re.sub(r\"args_work_dir\\s*=\\s*.*\", f\"args_work_dir = '{save_dir}'\", hparams_py_read)\n",
    "with open(\"/workspace/content/DiffSinger/utils/hparams.py\", \"w\") as f:\n",
    "    f.write(hparams_py_read)\n",
    "\n",
    "with open(\"/workspace/content/DiffSinger/utils/training_utils.py\", \"r\") as f:\n",
    "    training_utils_stuff = f.read()\n",
    "training_utils_stuff = re.sub(\"relative_path\\s*=\\s*.*\", \"relative_path = filepath.relative_to(Path('/workspace/content').resolve())\", training_utils_stuff)\n",
    "with open(\"/workspace/content/DiffSinger/utils/training_utils.py\", \"w\") as f:\n",
    "    f.write(training_utils_stuff)\n",
    "\n",
    "print(\"config updated! see below for config's information\")\n",
    "print(\"|\")\n",
    "print(\"|\")\n",
    "print(\"|\")\n",
    "print(f\"+++---{config_cap} {singer_type} TRAINING---+++\")\n",
    "print(\"|\")\n",
    "print(\"|\")\n",
    "print(\"|\")\n",
    "print(\"+++---user's settings---+++\")\n",
    "print(\"\\n\")\n",
    "print(f\"speaker name: {spk_names}\")\n",
    "print(\"\\n\")\n",
    "print(f\"data augmentation: {data_aug}\")\n",
    "print(\"\\n\")\n",
    "print(f\"pitch extractor: {f0_ext}\")\n",
    "print(\"\\n\")\n",
    "print(f\"binary data save directory: {binary_save_dir}\")\n",
    "print(\"\\n\")\n",
    "print(f\"your model will be saved to: {save_dir}\")\n",
    "print(\"\\n\")\n",
    "print(\"==========================================================================================\")\n",
    "print(\"\\n\")\n",
    "print(\"+++---other auto-defined settings---+++\")\n",
    "print(\"\\n\")\n",
    "print(f\"test files (auto selected): {random_ass_test_files}\")\n",
    "print(\"\\n\")\n",
    "print(\"dictionary (auto generated): custom_dict.txt\")\n",
    "print(\"\\n\")\n",
    "print(f\"max_batch_size: {batch_size}\")\n",
    "print(\"\\n\")\n",
    "print(\"==========================================================================================\")\n",
    "print(\"\\n\")\n",
    "print(\"if you don't like or disagree with any of these options,\")\n",
    "print(f\"you can go and edit the config at [/workspace/content/DiffSinger/configs/{config_type}.yaml]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab967b4-3834-4ce6-ba93-259d1abd6205",
   "metadata": {
    "id": "eexZl_OCDmQ3"
   },
   "source": [
    "# [Train_Acoustic_3] **Binarize the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3a8ea98-69ba-4800-9026-227b7dbcb8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/content/DiffSinger\n",
      "| Hparams chains:  ['configs/base.yaml', '/workspace/content/DiffSinger/configs/acoustic.yaml']\n",
      "| Hparams: \n",
      "\u001b[0;33mK_step\u001b[0m: 400, \u001b[0;33mK_step_infer\u001b[0m: 400, \u001b[0;33maccumulate_grad_batches\u001b[0m: 1, \u001b[0;33maudio_num_mel_bins\u001b[0m: 128, \u001b[0;33maudio_sample_rate\u001b[0m: 44100, \n",
      "\u001b[0;33maugmentation_args\u001b[0m: {'fixed_pitch_shifting': {'enabled': False, 'scale': 0.5, 'targets': [-5.0, 5.0]}, 'random_pitch_shifting': {'enabled': True, 'range': [-5.0, 5.0], 'scale': 0.75}, 'random_time_stretching': {'enabled': True, 'range': [0.5, 2.0], 'scale': 0.75}}, \u001b[0;33mbase_config\u001b[0m: [], \u001b[0;33mbinarization_args\u001b[0m: {'num_workers': 0, 'shuffle': True}, \u001b[0;33mbinarizer_cls\u001b[0m: preprocessing.acoustic_binarizer.AcousticBinarizer, \u001b[0;33mbinary_data_dir\u001b[0m: /workspace/content/DiffSinger/MyModel_Acoustic/binary, \n",
      "\u001b[0;33mbreathiness_smooth_width\u001b[0m: 0.12, \u001b[0;33mclip_grad_norm\u001b[0m: 1, \u001b[0;33mdataloader_prefetch_factor\u001b[0m: 2, \u001b[0;33mdataset_size_key\u001b[0m: lengths, \u001b[0;33mdictionary\u001b[0m: dictionaries/custom_dict.txt, \n",
      "\u001b[0;33mdiff_accelerator\u001b[0m: ddim, \u001b[0;33mdiff_decoder_type\u001b[0m: wavenet, \u001b[0;33mdiff_loss_type\u001b[0m: l2, \u001b[0;33mdiff_speedup\u001b[0m: 10, \u001b[0;33mdilation_cycle_length\u001b[0m: 4, \n",
      "\u001b[0;33mdropout\u001b[0m: 0.1, \u001b[0;33mds_workers\u001b[0m: 4, \u001b[0;33menc_ffn_kernel_size\u001b[0m: 9, \u001b[0;33menc_layers\u001b[0m: 4, \u001b[0;33menergy_smooth_width\u001b[0m: 0.12, \n",
      "\u001b[0;33mexp_name\u001b[0m: , \u001b[0;33mf0_embed_type\u001b[0m: continuous, \u001b[0;33mf0_max\u001b[0m: 1100, \u001b[0;33mf0_min\u001b[0m: 65, \u001b[0;33mffn_act\u001b[0m: gelu, \n",
      "\u001b[0;33mfft_size\u001b[0m: 2048, \u001b[0;33mfinetune_ckpt_path\u001b[0m: None, \u001b[0;33mfinetune_enabled\u001b[0m: False, \u001b[0;33mfinetune_ignored_params\u001b[0m: ['model.fs2.encoder.embed_tokens', 'model.fs2.txt_embed', 'model.fs2.spk_embed'], \u001b[0;33mfinetune_strict_shapes\u001b[0m: True, \n",
      "\u001b[0;33mfmax\u001b[0m: 16000, \u001b[0;33mfmin\u001b[0m: 40, \u001b[0;33mfreezing_enabled\u001b[0m: False, \u001b[0;33mfrozen_params\u001b[0m: [], \u001b[0;33mhidden_size\u001b[0m: 256, \n",
      "\u001b[0;33mhop_size\u001b[0m: 512, \u001b[0;33minfer\u001b[0m: False, \u001b[0;33mlambda_aux_mel_loss\u001b[0m: 0.2, \u001b[0;33mlog_interval\u001b[0m: 100, \u001b[0;33mlr_scheduler_args\u001b[0m: {'gamma': 0.5, 'scheduler_cls': 'torch.optim.lr_scheduler.StepLR', 'step_size': 30000}, \n",
      "\u001b[0;33mmax_batch_frames\u001b[0m: 50000, \u001b[0;33mmax_batch_size\u001b[0m: 12, \u001b[0;33mmax_beta\u001b[0m: 0.02, \u001b[0;33mmax_updates\u001b[0m: 200000, \u001b[0;33mmax_val_batch_frames\u001b[0m: 60000, \n",
      "\u001b[0;33mmax_val_batch_size\u001b[0m: 1, \u001b[0;33mmel_vmax\u001b[0m: 1.5, \u001b[0;33mmel_vmin\u001b[0m: -6.0, \u001b[0;33mnccl_p2p\u001b[0m: True, \u001b[0;33mnum_ckpt_keep\u001b[0m: 5, \n",
      "\u001b[0;33mnum_heads\u001b[0m: 2, \u001b[0;33mnum_sanity_val_steps\u001b[0m: 1, \u001b[0;33mnum_spk\u001b[0m: 1, \u001b[0;33mnum_valid_plots\u001b[0m: 10, \u001b[0;33moptimizer_args\u001b[0m: {'beta1': 0.9, 'beta2': 0.98, 'lr': 0.0006, 'optimizer_cls': 'torch.optim.AdamW', 'weight_decay': 0}, \n",
      "\u001b[0;33mpe\u001b[0m: rmvpe, \u001b[0;33mpe_ckpt\u001b[0m: checkpoints/rmvpe/model.pt, \u001b[0;33mpermanent_ckpt_interval\u001b[0m: 20000, \u001b[0;33mpermanent_ckpt_start\u001b[0m: 120000, \u001b[0;33mpl_trainer_accelerator\u001b[0m: auto, \n",
      "\u001b[0;33mpl_trainer_devices\u001b[0m: auto, \u001b[0;33mpl_trainer_num_nodes\u001b[0m: 1, \u001b[0;33mpl_trainer_precision\u001b[0m: 16-mixed, \u001b[0;33mpl_trainer_strategy\u001b[0m: {'find_unused_parameters': False, 'name': 'auto', 'process_group_backend': 'nccl'}, \u001b[0;33mraw_data_dir\u001b[0m: ['/workspace/content/raw_data/diffsinger_db/data'], \n",
      "\u001b[0;33mrel_pos\u001b[0m: True, \u001b[0;33mresidual_channels\u001b[0m: 512, \u001b[0;33mresidual_layers\u001b[0m: 20, \u001b[0;33msampler_frame_count_grid\u001b[0m: 6, \u001b[0;33mschedule_type\u001b[0m: linear, \n",
      "\u001b[0;33mshallow_diffusion_args\u001b[0m: {'aux_decoder_arch': 'convnext', 'aux_decoder_args': {'dropout_rate': 0.1, 'kernel_size': 7, 'num_channels': 512, 'num_layers': 6}, 'aux_decoder_grad': 0.1, 'train_aux_decoder': True, 'train_diffusion': True, 'val_gt_start': True}, \u001b[0;33msort_by_len\u001b[0m: True, \u001b[0;33mspeakers\u001b[0m: ['data'], \u001b[0;33mspec_max\u001b[0m: [0], \u001b[0;33mspec_min\u001b[0m: [-5], \n",
      "\u001b[0;33mspk_ids\u001b[0m: [], \u001b[0;33mtask_cls\u001b[0m: training.acoustic_task.AcousticTask, \u001b[0;33mtension_smooth_width\u001b[0m: 0.12, \u001b[0;33mtest_prefixes\u001b[0m: ['tiger_004_seg006', 'tiger_027_seg002', 'tiger_021_seg011'], \u001b[0;33mtimesteps\u001b[0m: 1000, \n",
      "\u001b[0;33muse_breathiness_embed\u001b[0m: True, \u001b[0;33muse_energy_embed\u001b[0m: True, \u001b[0;33muse_key_shift_embed\u001b[0m: True, \u001b[0;33muse_pos_embed\u001b[0m: True, \u001b[0;33muse_shallow_diffusion\u001b[0m: True, \n",
      "\u001b[0;33muse_speed_embed\u001b[0m: True, \u001b[0;33muse_spk_id\u001b[0m: False, \u001b[0;33muse_tension_embed\u001b[0m: True, \u001b[0;33muse_voicing_embed\u001b[0m: True, \u001b[0;33mval_check_interval\u001b[0m: 2000, \n",
      "\u001b[0;33mval_with_vocoder\u001b[0m: True, \u001b[0;33mvocoder\u001b[0m: NsfHifiGAN, \u001b[0;33mvocoder_ckpt\u001b[0m: checkpoints/nsf_hifigan_44.1k_hop512_128bin_2024.02/model.ckpt, \u001b[0;33mvoicing_smooth_width\u001b[0m: 0.12, \u001b[0;33mwin_size\u001b[0m: 2048, \n",
      "\u001b[0;33mwork_dir\u001b[0m: /workspace/content/DiffSinger/MyModel_Acoustic, \n",
      "| Binarizer:  <class 'preprocessing.acoustic_binarizer.AcousticBinarizer'>\n",
      "| spk_map:  {'data': 0}\n",
      "| load phoneme set: ['AP', 'SP', 'aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ay', 'b', 'ch', 'cl', 'd', 'dh', 'dr', 'dx', 'eh', 'er', 'ey', 'f', 'g', 'hh', 'ih', 'iy', 'jh', 'k', 'l', 'm', 'n', 'ng', 'ow', 'oy', 'p', 'q', 'r', 's', 'sh', 't', 'th', 'tr', 'uh', 'uw', 'v', 'vf', 'vtrash', 'w', 'y', 'z', 'zh']\n",
      "===== Phoneme Distribution Summary =====\n",
      "'AP': 675, 'SP': 486, 'aa': 401, 'ae': 228, 'ah': 290, 'ao': 165, 'aw': 99, 'ax': 581, 'ay': 291, 'b': 224,\n",
      "'ch': 72, 'cl': 100, 'd': 217, 'dh': 199, 'dr': 2, 'dx': 201, 'eh': 343, 'er': 205, 'ey': 191, 'f': 156,\n",
      "'g': 137, 'hh': 164, 'ih': 508, 'iy': 369, 'jh': 51, 'k': 273, 'l': 402, 'm': 388, 'n': 749, 'ng': 114,\n",
      "'ow': 250, 'oy': 7, 'p': 174, 'q': 158, 'r': 376, 's': 388, 'sh': 44, 't': 404, 'th': 59, 'tr': 3,\n",
      "'uh': 56, 'uw': 309, 'v': 155, 'vf': 37, 'vtrash': 2, 'w': 216, 'y': 201, 'z': 167, 'zh': 5\n",
      "| save summary to '/workspace/content/DiffSinger/MyModel_Acoustic/binary/phoneme_distribution.jpg'\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:04<00:00,  1.47s/it]\n",
      "| valid total duration: 22.28s\n",
      "| valid respective duration: data=22.28s\n",
      "100%|█████████████████████████████████████████| 390/390 [05:10<00:00,  1.26it/s]\n",
      "| train total duration (before augmentation): 2252.07s\n",
      "| train respective duration (before augmentation): data=2252.07s\n",
      "| train total duration (after augmentation): 5718.13s (2.54x)\n",
      "| train respective duration (after augmentation): data=5718.13s\n"
     ]
    }
   ],
   "source": [
    "#@markdown # Preprocess data\n",
    "import os\n",
    "\n",
    "# idk i just feel like 800 is a lil low for some people part 2\n",
    "new_f0_max = 1600\n",
    "og_script = \"/workspace/content/DiffSinger/utils/binarizer_utils.py\"\n",
    "with open(og_script, 'r') as file:\n",
    "    mate = file.read()\n",
    "up_f0_val = mate.replace(\"f0_max = 800\", f\"f0_max = {new_f0_max}\")\n",
    "with open(og_script, 'w') as file:\n",
    "    file.write(up_f0_val)\n",
    "\n",
    "training_config = f\"/workspace/content/DiffSinger/configs/{config_type}.yaml\"\n",
    "%cd /workspace/content/DiffSinger\n",
    "os.environ['PYTHONPATH']='.'\n",
    "!CUDA_VISIBLE_DEVICES=0 python /workspace/content/DiffSinger/scripts/binarize.py --config {training_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5be87de3-cfcc-42d4-bdc0-4429188963bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/content/DiffSinger\n",
      "| Hparams chains:  ['configs/base.yaml', '/workspace/content/DiffSinger/configs/acoustic.yaml']\n",
      "| Hparams: \n",
      "\u001b[0;33mK_step\u001b[0m: 400, \u001b[0;33mK_step_infer\u001b[0m: 400, \u001b[0;33maccumulate_grad_batches\u001b[0m: 1, \u001b[0;33maudio_num_mel_bins\u001b[0m: 128, \u001b[0;33maudio_sample_rate\u001b[0m: 44100, \n",
      "\u001b[0;33maugmentation_args\u001b[0m: {'fixed_pitch_shifting': {'enabled': False, 'scale': 0.5, 'targets': [-5.0, 5.0]}, 'random_pitch_shifting': {'enabled': True, 'range': [-5.0, 5.0], 'scale': 0.75}, 'random_time_stretching': {'enabled': True, 'range': [0.5, 2.0], 'scale': 0.75}}, \u001b[0;33mbase_config\u001b[0m: ['configs/base.yaml'], \u001b[0;33mbinarization_args\u001b[0m: {'num_workers': 0, 'shuffle': True}, \u001b[0;33mbinarizer_cls\u001b[0m: preprocessing.acoustic_binarizer.AcousticBinarizer, \u001b[0;33mbinary_data_dir\u001b[0m: /workspace/content/DiffSinger/MyModel_Acoustic/binary, \n",
      "\u001b[0;33mbreathiness_smooth_width\u001b[0m: 0.12, \u001b[0;33mclip_grad_norm\u001b[0m: 1, \u001b[0;33mdataloader_prefetch_factor\u001b[0m: 2, \u001b[0;33mdataset_size_key\u001b[0m: lengths, \u001b[0;33mdictionary\u001b[0m: dictionaries/custom_dict.txt, \n",
      "\u001b[0;33mdiff_accelerator\u001b[0m: ddim, \u001b[0;33mdiff_decoder_type\u001b[0m: wavenet, \u001b[0;33mdiff_loss_type\u001b[0m: l2, \u001b[0;33mdiff_speedup\u001b[0m: 10, \u001b[0;33mdilation_cycle_length\u001b[0m: 4, \n",
      "\u001b[0;33mdropout\u001b[0m: 0.1, \u001b[0;33mds_workers\u001b[0m: 4, \u001b[0;33menc_ffn_kernel_size\u001b[0m: 9, \u001b[0;33menc_layers\u001b[0m: 4, \u001b[0;33menergy_smooth_width\u001b[0m: 0.12, \n",
      "\u001b[0;33mexp_name\u001b[0m: $/workspace/content/DiffSinger, \u001b[0;33mf0_embed_type\u001b[0m: continuous, \u001b[0;33mf0_max\u001b[0m: 1100, \u001b[0;33mf0_min\u001b[0m: 65, \u001b[0;33mffn_act\u001b[0m: gelu, \n",
      "\u001b[0;33mfft_size\u001b[0m: 2048, \u001b[0;33mfinetune_ckpt_path\u001b[0m: None, \u001b[0;33mfinetune_enabled\u001b[0m: False, \u001b[0;33mfinetune_ignored_params\u001b[0m: ['model.fs2.encoder.embed_tokens', 'model.fs2.txt_embed', 'model.fs2.spk_embed'], \u001b[0;33mfinetune_strict_shapes\u001b[0m: True, \n",
      "\u001b[0;33mfmax\u001b[0m: 16000, \u001b[0;33mfmin\u001b[0m: 40, \u001b[0;33mfreezing_enabled\u001b[0m: False, \u001b[0;33mfrozen_params\u001b[0m: [], \u001b[0;33mhidden_size\u001b[0m: 256, \n",
      "\u001b[0;33mhop_size\u001b[0m: 512, \u001b[0;33minfer\u001b[0m: False, \u001b[0;33mlambda_aux_mel_loss\u001b[0m: 0.2, \u001b[0;33mlog_interval\u001b[0m: 100, \u001b[0;33mlr_scheduler_args\u001b[0m: {'gamma': 0.5, 'scheduler_cls': 'torch.optim.lr_scheduler.StepLR', 'step_size': 30000}, \n",
      "\u001b[0;33mmax_batch_frames\u001b[0m: 50000, \u001b[0;33mmax_batch_size\u001b[0m: 12, \u001b[0;33mmax_beta\u001b[0m: 0.02, \u001b[0;33mmax_updates\u001b[0m: 200000, \u001b[0;33mmax_val_batch_frames\u001b[0m: 60000, \n",
      "\u001b[0;33mmax_val_batch_size\u001b[0m: 1, \u001b[0;33mmel_vmax\u001b[0m: 1.5, \u001b[0;33mmel_vmin\u001b[0m: -6.0, \u001b[0;33mnccl_p2p\u001b[0m: True, \u001b[0;33mnum_ckpt_keep\u001b[0m: 5, \n",
      "\u001b[0;33mnum_heads\u001b[0m: 2, \u001b[0;33mnum_sanity_val_steps\u001b[0m: 1, \u001b[0;33mnum_spk\u001b[0m: 1, \u001b[0;33mnum_valid_plots\u001b[0m: 10, \u001b[0;33moptimizer_args\u001b[0m: {'beta1': 0.9, 'beta2': 0.98, 'lr': 0.0006, 'optimizer_cls': 'torch.optim.AdamW', 'weight_decay': 0}, \n",
      "\u001b[0;33mpe\u001b[0m: rmvpe, \u001b[0;33mpe_ckpt\u001b[0m: checkpoints/rmvpe/model.pt, \u001b[0;33mpermanent_ckpt_interval\u001b[0m: 20000, \u001b[0;33mpermanent_ckpt_start\u001b[0m: 120000, \u001b[0;33mpl_trainer_accelerator\u001b[0m: auto, \n",
      "\u001b[0;33mpl_trainer_devices\u001b[0m: auto, \u001b[0;33mpl_trainer_num_nodes\u001b[0m: 1, \u001b[0;33mpl_trainer_precision\u001b[0m: 16-mixed, \u001b[0;33mpl_trainer_strategy\u001b[0m: {'find_unused_parameters': False, 'name': 'auto', 'process_group_backend': 'nccl'}, \u001b[0;33mraw_data_dir\u001b[0m: ['/workspace/content/raw_data/diffsinger_db/data'], \n",
      "\u001b[0;33mrel_pos\u001b[0m: True, \u001b[0;33mresidual_channels\u001b[0m: 512, \u001b[0;33mresidual_layers\u001b[0m: 20, \u001b[0;33msampler_frame_count_grid\u001b[0m: 6, \u001b[0;33mschedule_type\u001b[0m: linear, \n",
      "\u001b[0;33mshallow_diffusion_args\u001b[0m: {'aux_decoder_arch': 'convnext', 'aux_decoder_args': {'dropout_rate': 0.1, 'kernel_size': 7, 'num_channels': 512, 'num_layers': 6}, 'aux_decoder_grad': 0.1, 'train_aux_decoder': True, 'train_diffusion': True, 'val_gt_start': True}, \u001b[0;33msort_by_len\u001b[0m: True, \u001b[0;33mspeakers\u001b[0m: ['data'], \u001b[0;33mspec_max\u001b[0m: [0], \u001b[0;33mspec_min\u001b[0m: [-5], \n",
      "\u001b[0;33mspk_ids\u001b[0m: [], \u001b[0;33mtask_cls\u001b[0m: training.acoustic_task.AcousticTask, \u001b[0;33mtension_smooth_width\u001b[0m: 0.12, \u001b[0;33mtest_prefixes\u001b[0m: ['tiger_023_seg000', 'tiger_019_seg002', 'tiger_027_seg011'], \u001b[0;33mtimesteps\u001b[0m: 1000, \n",
      "\u001b[0;33muse_breathiness_embed\u001b[0m: True, \u001b[0;33muse_energy_embed\u001b[0m: True, \u001b[0;33muse_key_shift_embed\u001b[0m: True, \u001b[0;33muse_pos_embed\u001b[0m: True, \u001b[0;33muse_shallow_diffusion\u001b[0m: True, \n",
      "\u001b[0;33muse_speed_embed\u001b[0m: True, \u001b[0;33muse_spk_id\u001b[0m: False, \u001b[0;33muse_tension_embed\u001b[0m: True, \u001b[0;33muse_voicing_embed\u001b[0m: True, \u001b[0;33mval_check_interval\u001b[0m: 2000, \n",
      "\u001b[0;33mval_with_vocoder\u001b[0m: True, \u001b[0;33mvocoder\u001b[0m: NsfHifiGAN, \u001b[0;33mvocoder_ckpt\u001b[0m: checkpoints/nsf_hifigan_44.1k_hop512_128bin_2024.02/model.ckpt, \u001b[0;33mvoicing_smooth_width\u001b[0m: 0.12, \u001b[0;33mwin_size\u001b[0m: 2048, \n",
      "\u001b[0;33mwork_dir\u001b[0m: /workspace/content/DiffSinger/MyModel_Acoustic, \n",
      "| load phoneme set: ['AP', 'SP', 'aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ay', 'b', 'ch', 'cl', 'd', 'dh', 'dr', 'dx', 'eh', 'er', 'ey', 'f', 'g', 'hh', 'ih', 'iy', 'jh', 'k', 'l', 'm', 'n', 'ng', 'ow', 'oy', 'p', 'q', 'r', 's', 'sh', 't', 'th', 'tr', 'uh', 'uw', 'v', 'vf', 'vtrash', 'w', 'y', 'z', 'zh']\n",
      "| model Arch:  DiffSingerAcoustic(\n",
      "  (fs2): FastSpeech2Acoustic(\n",
      "    (txt_embed): NormalInitEmbedding(50, 256, padding_idx=0)\n",
      "    (dur_embed): XavierUniformInitLinear(in_features=1, out_features=256, bias=True)\n",
      "    (encoder): FastSpeech2Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayer(\n",
      "          (op): EncSALayer(\n",
      "            (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=False)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (ffn): TransformerFFNLayer(\n",
      "              (ffn_1): Conv1d(256, 1024, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "              (act_fn): GELU(approximate='none')\n",
      "              (ffn_2): XavierUniformInitLinear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (embed_positions): RelPositionalEncoding(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (pitch_embed): XavierUniformInitLinear(in_features=1, out_features=256, bias=True)\n",
      "    (variance_embeds): ModuleDict(\n",
      "      (energy): XavierUniformInitLinear(in_features=1, out_features=256, bias=True)\n",
      "      (breathiness): XavierUniformInitLinear(in_features=1, out_features=256, bias=True)\n",
      "      (voicing): XavierUniformInitLinear(in_features=1, out_features=256, bias=True)\n",
      "      (tension): XavierUniformInitLinear(in_features=1, out_features=256, bias=True)\n",
      "    )\n",
      "    (key_shift_embed): XavierUniformInitLinear(in_features=1, out_features=256, bias=True)\n",
      "    (speed_embed): XavierUniformInitLinear(in_features=1, out_features=256, bias=True)\n",
      "  )\n",
      "  (aux_decoder): AuxDecoderAdaptor(\n",
      "    (decoder): ConvNeXtDecoder(\n",
      "      (inconv): Conv1d(256, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (conv): ModuleList(\n",
      "        (0-5): 6 x ConvNeXtBlock(\n",
      "          (dwconv): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop_path): Identity()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (outconv): Conv1d(512, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "    )\n",
      "  )\n",
      "  (diffusion): GaussianDiffusion(\n",
      "    (denoise_fn): WaveNet(\n",
      "      (input_projection): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
      "      (diffusion_embedding): SinusoidalPosEmb()\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): Mish()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (residual_layers): ModuleList(\n",
      "        (0): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (2): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (3): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (4): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (5): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (6): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (7): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (8): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (9): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (10): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (11): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (12): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (13): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (14): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (15): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (16): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (17): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (18): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (19): ResidualBlock(\n",
      "          (dilated_conv): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "          (diffusion_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (conditioner_projection): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (output_projection): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "      (skip_projection): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      (output_projection): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| Load HifiGAN: checkpoints/nsf_hifigan_44.1k_hop512_128bin_2024.02/model.ckpt\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Removing weight norm...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "| Copied spk map to /workspace/content/DiffSinger/MyModel_Acoustic/spk_map.json.\n",
      "| Copied dictionary to /workspace/content/DiffSinger/MyModel_Acoustic/dictionary.txt.\n",
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory /workspace/content/DiffSinger/MyModel_Acoustic exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type               | Params\n",
      "----------------------------------------------------\n",
      "0 | model        | DiffSingerAcoustic | 80.6 M\n",
      "1 | aux_mel_loss | L1Loss             | 0     \n",
      "2 | mel_loss     | DiffusionNoiseLoss | 0     \n",
      "----------------------------------------------------\n",
      "80.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "80.6 M    Total params\n",
      "322.258   Total estimated model params size (MB)\n",
      "Epoch 24:  39%|▍| 32/82 [00:06<00:10,  4.83it/s, aux_mel_loss=0.02748, mel_loss=\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 1/3 [00:06<00:13,  0.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 2/3 [00:10<00:05,  0.18it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 3/3 [00:16<00:00,  0.18it/s]\u001b[A\n",
      "Epoch 24:  39%|▍| 32/82 [00:23<00:36,  1.38it/s, aux_mel_loss=0.02748, mel_loss=\u001b[ACheckpoint DiffSinger/MyModel_Acoustic/model_ckpt_steps_2000.ckpt saved.\n",
      "Epoch 48:  78%|▊| 64/82 [00:11<00:03,  5.60it/s, aux_mel_loss=0.02593, mel_loss=\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 1/3 [00:06<00:12,  0.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 2/3 [00:10<00:05,  0.19it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 3/3 [00:16<00:00,  0.19it/s]\u001b[A\n",
      "Epoch 48:  78%|▊| 64/82 [00:27<00:07,  2.32it/s, aux_mel_loss=0.02593, mel_loss=\u001b[ACheckpoint DiffSinger/MyModel_Acoustic/model_ckpt_steps_4000.ckpt saved.\n",
      "Epoch 73:  17%|▏| 14/82 [00:03<00:17,  4.00it/s, aux_mel_loss=0.02342, mel_loss=\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 1/3 [00:06<00:12,  0.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 2/3 [00:10<00:05,  0.19it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 3/3 [00:15<00:00,  0.19it/s]\u001b[A\n",
      "Epoch 73:  17%|▏| 14/82 [00:19<01:34,  0.72it/s, aux_mel_loss=0.02342, mel_loss=\u001b[ACheckpoint DiffSinger/MyModel_Acoustic/model_ckpt_steps_6000.ckpt saved.\n",
      "Epoch 89:  40%|▍| 33/82 [00:06<00:10,  4.83it/s, aux_mel_loss=0.01761, mel_loss=^C\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "Epoch 89:  40%|▍| 33/82 [00:07<00:10,  4.65it/s, aux_mel_loss=0.0184, mel_loss=0\n"
     ]
    }
   ],
   "source": [
    "#@markdown #Train your model\n",
    "%cd /workspace/content/DiffSinger\n",
    "import re\n",
    "import os\n",
    "import yaml\n",
    "#@markdown ___\n",
    "\n",
    "#@markdown <font size=\"-1.5\"> Step interval of when your model will be validate and save\n",
    "save_interval = 2000 #@param {type:\"slider\", min:100, max:10000, step:100}\n",
    "\n",
    "#@markdown ___\n",
    "\n",
    "#@markdown ###**Only edit this section if you want to resume training**\n",
    "resume_training = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown <font size=\"-1.5\"> path to the config you got from training\n",
    "re_config_path = \"/workspace/content/DiffSinger/MyModel_Acoustic\" #@param {type:\"string\"}\n",
    "model_dir = os.path.dirname(re_config_path)\n",
    "save_dir = model_dir\n",
    "if resume_training:\n",
    "    with open(\"/workspace/content/DiffSinger/utils/hparams.py\", \"r\") as f:\n",
    "        hparams_py_read = f.read()\n",
    "    hparams_py_read = re.sub(r\"args_work_dir\\s*=\\s*.*\", f\"args_work_dir = '{save_dir}'\", hparams_py_read)\n",
    "    with open(\"/workspace/content/DiffSinger/utils/hparams.py\", \"w\") as f:\n",
    "        f.write(hparams_py_read)\n",
    "    with open(\"/workspace/content/DiffSinger/utils/training_utils.py\", \"r\") as f:\n",
    "        training_utils_stuff = f.read()\n",
    "    training_utils_stuff = re.sub(\"relative_path\\s*=\\s*.*\", \"relative_path = filepath.relative_to(Path('/workspace/content').resolve())\", training_utils_stuff)\n",
    "    with open(\"/workspace/content/DiffSinger/utils/training_utils.py\", \"w\") as f:\n",
    "        f.write(training_utils_stuff)\n",
    "\n",
    "    config_path = re_config_path\n",
    "    log_dir = save_dir\n",
    "\n",
    "    !cp {model_dir}/dictionary.txt /workspace/content/DiffSinger/dictionaries/custom_dict.txt\n",
    "\n",
    "else:\n",
    "    config_path = training_config\n",
    "    log_dir = conf_dir\n",
    "\n",
    "with open(config_path, \"r\") as config:\n",
    "    ehe = yaml.safe_load(config)\n",
    "ehe[\"val_check_interval\"] = save_interval\n",
    "with open(config_path, \"w\") as config:\n",
    "    yaml.dump(ehe, config)\n",
    "\n",
    "# logs = log_dir\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir {logs}/lightning_logs\n",
    "\n",
    "!python /workspace/content/DiffSinger/scripts/train.py --config {config_path} --exp_name ${save_dir} --reset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
